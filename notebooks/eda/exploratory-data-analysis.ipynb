{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:21:56.281829Z",
     "start_time": "2024-09-22T12:21:39.910549Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sqlalchemy\n",
    "from transformers import pipeline, RobertaTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import language_tool_python\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "sys.path.append('../')\n",
    "from credentials import credentials\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:22:41.464532Z",
     "start_time": "2024-09-22T12:22:41.447977Z"
    }
   },
   "source": [
    "connector_string = f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}/AuthenticAI'\n",
    "db_engine = sqlalchemy.create_engine(connector_string,echo=True)\n",
    "db_conn = db_engine.connect()"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Credentials' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m connector_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmysql+mysqlconnector://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/AuthenticAI\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      2\u001B[0m db_engine \u001B[38;5;241m=\u001B[39m sqlalchemy\u001B[38;5;241m.\u001B[39mcreate_engine(connector_string,echo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m db_conn \u001B[38;5;241m=\u001B[39m db_engine\u001B[38;5;241m.\u001B[39mconnect()\n",
      "\u001B[0;31mTypeError\u001B[0m: 'Credentials' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:21:56.473077Z",
     "start_time": "2024-09-22T12:21:56.472994Z"
    }
   },
   "source": [
    "student_written_count = [i[0] for i in db_conn.execute(sqlalchemy.text('select count(*) from essays where essays.LLM_written = 0;'))][0]\n",
    "llm_written_count = [i[0] for i in db_conn.execute(sqlalchemy.text('select count(*) from essays where essays.LLM_written = 1;'))][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:22:32.868974Z",
     "start_time": "2024-09-22T12:22:32.847075Z"
    }
   },
   "source": [
    "classes = ['human Written','LLM Written']\n",
    "data = [student_written_count, llm_written_count]\n",
    "plt.pie(x=data,labels=classes,autopct='%.0f%%')\n",
    "plt.title('Percentage of Data per Class')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_written_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m classes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman Written\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLLM Written\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 2\u001B[0m data \u001B[38;5;241m=\u001B[39m [\u001B[43mstudent_written_count\u001B[49m, llm_written_count]\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mpie(x\u001B[38;5;241m=\u001B[39mdata,labels\u001B[38;5;241m=\u001B[39mclasses,autopct\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%.0f\u001B[39;00m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPercentage of Data per Class\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'student_written_count' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "word_counts = pd.DataFrame([i for i in db_conn.execute(sqlalchemy.text('select word_count, LLM_written from essays;'))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data=word_counts,x='word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Word Counts for Each Class')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.title('Distribution of Word Counts for Each Class')\n",
    "sns.histplot(data=word_counts,x='word_count',hue='LLM_written')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student Written Essay Descriptive Statistics')\n",
    "print(word_counts[word_counts['LLM_written'] == 0]['word_count'].describe())\n",
    "print()\n",
    "print('LLM Written Essay Descriptive Statistics')\n",
    "print(word_counts[word_counts['LLM_written'] == 1]['word_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "detector = pipeline(\"text-classification\",\"roberta-base-openai-detector\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "text_and_labels = pd.DataFrame([i for i in db_conn.execute(sqlalchemy.text('select essay, LLM_written from essays;'))])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def num_of_tokens(text:str) -> int:\n",
    "    tokenized_text = tokenizer(text)['input_ids']\n",
    "    return len(tokenized_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "text_and_labels['token_count'] = text_and_labels['essay'].progress_apply(num_of_tokens)\n",
    "text_and_labels.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "valid_examples = text_and_labels[text_and_labels['token_count'] <= 512]\n",
    "valid_examples.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "valid_examples.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "_, sample = train_test_split(valid_examples,test_size=1000,random_state=42,shuffle=True,stratify=valid_examples['LLM_written'])\n",
    "sample['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "predictions = detector(sample['essay'].tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "pred_list = [0 if pred['label'] == 'Real' else 1 for pred in predictions]\n",
    "sample['predictions'] = pred_list\n",
    "sample.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "accuracy = accuracy_score(sample['LLM_written'],sample['predictions'])\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "matrix = confusion_matrix(sample['LLM_written'],sample['predictions'])\n",
    "display = ConfusionMatrixDisplay(matrix)\n",
    "display.plot()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print(f'P(Y = 0 | X = student written essay) = {matrix[0][0] / (matrix[0][0] + matrix[0][1])}')\n",
    "print(f'P(Y = 1 | X = LLM written essay) = {matrix[1][1] / (matrix[1][1] + matrix[1][0])}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tool = language_tool_python.LanguageTool('en-US',config={'cacheSize': 1000})",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def grammer_error_count(text:str) -> int:\n",
    "    errors = tool.check(text)\n",
    "    return len(errors)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_and_labels = pd.DataFrame([i for i in db_conn.execute(sqlalchemy.text('select essay, LLM_written from essays;'))])\n",
    "text_and_labels.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, sample = train_test_split(text_and_labels,test_size=5000,random_state=42,shuffle=True,stratify=text_and_labels['LLM_written'])\n",
    "sample['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample['grammar_error_count'] = sample['essay'].progress_apply(grammer_error_count)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "student = sample[sample['LLM_written'] == 0]\n",
    "llm_written = sample[sample['LLM_written'] == 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student Grammatical Errors')\n",
    "print(student['grammar_error_count'].describe())\n",
    "print()\n",
    "print('LLM Grammatical Errors')\n",
    "print(llm_written['grammar_error_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.title('Average Number of Grammar Mistakes per Class')\n",
    "plot = sns.barplot(data=sample,x='LLM_written',y='grammar_error_count',errorbar=None)\n",
    "for i in plot.containers:\n",
    "    plot.bar_label(i,)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db_conn.close()\n",
    "db_engine.dispose()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
