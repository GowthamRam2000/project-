{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:21.861629Z",
     "start_time": "2024-09-22T12:22:56.071432Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "import spacy\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from credentials import credentials\n",
    "\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:22.071193Z",
     "start_time": "2024-09-22T12:23:21.862495Z"
    }
   },
   "source": [
    "\n",
    "connector_string = f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}/AuthenticAI'\n",
    "db_engine = sqlalchemy.create_engine(connector_string,echo=True)\n",
    "\n",
    "db_conn = db_engine.connect()"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Credentials' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Creating the database engine \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m connector_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmysql+mysqlconnector://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/AuthenticAI\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m db_engine \u001B[38;5;241m=\u001B[39m sqlalchemy\u001B[38;5;241m.\u001B[39mcreate_engine(connector_string,echo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Connecting to the database\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'Credentials' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:22.074372Z",
     "start_time": "2024-09-22T12:23:22.074308Z"
    }
   },
   "source": "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_unique_words(text:str) -> int:\n",
    "    tokenized = set(tokenizer.tokenize(text))\n",
    "    return len(tokenized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = pd.DataFrame([row for row in db_conn.execute(sqlalchemy.text('select * from essays;'))])\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data['unique_word_count'] = data['essay'].progress_apply(get_unique_words)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='unique_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts for Each Class')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student Unique Words')\n",
    "print(data[data['LLM_written'] == 0]['unique_word_count'].describe())\n",
    "print()\n",
    "print('LLM Unique Words')\n",
    "print(data[data['LLM_written'] == 1]['unique_word_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data['unique_to_total'] = data['unique_word_count'] / data['word_count']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='unique_to_total',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts to Total Words Ratio for Each Class')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student Unique Words to Total')\n",
    "print(data[data['LLM_written'] == 0]['unique_to_total'].describe())\n",
    "print()\n",
    "print('LLM Unique Word to Total')\n",
    "print(data[data['LLM_written'] == 1]['unique_to_total'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "smaller_word_count = data[data['word_count'] <= 400]\n",
    "smaller_word_count['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='unique_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts for Each Class for Essays <= 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='unique_to_total',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts to Total Word Counts for Each Class for Essays <= 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def stop_word_count(text:str) -> int:\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    count = 0\n",
    "\n",
    "    for word in tokenized:\n",
    "        if word in stop_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data['stop_word_count'] = data['essay'].progress_apply(stop_word_count)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='stop_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student Stop Words')\n",
    "print(data[data['LLM_written'] == 0]['stop_word_count'].describe())\n",
    "print()\n",
    "print('Student Stop Word')\n",
    "print(data[data['LLM_written'] == 1]['stop_word_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data['stop_word_ratio'] = data['stop_word_count'] / data['word_count']",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='stop_word_ratio',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Word Ratio')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "smaller_word_count = data[data['word_count'] <= 400]\n",
    "smaller_word_count['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='stop_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words for Essays less than 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='stop_word_ratio',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words/Total Words for Essays less than 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pytorch_tokenizer = get_tokenizer('spacy',language='en_core_web_sm')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def count_punc(text: str) -> int:\n",
    "    tokenized_text = pytorch_tokenizer(text)\n",
    "    count_q = 0\n",
    "    count_ex = 0\n",
    "    count_semi = 0\n",
    "    count_col = 0\n",
    "    for token in tokenized_text:\n",
    "        if token == \"?\":\n",
    "            count_q += 1\n",
    "        elif token == \"!\":\n",
    "            count_ex += 1\n",
    "        elif token == \";\":\n",
    "            count_semi += 1\n",
    "        elif token == \":\":\n",
    "            count_col += 1\n",
    "    \n",
    "    return count_q, count_ex,count_semi, count_col"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "counts = data['essay'].progress_apply(count_punc)\n",
    "data['count_question'] = [row[0] for row in counts]\n",
    "data['count_exclamation'] = [row[1] for row in counts]\n",
    "data['count_semi'] = [row[2] for row in counts]\n",
    "data['count_colon'] = [row[3] for row in counts]\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student')\n",
    "print(data[data['LLM_written'] == 0][['count_question','count_exclamation','count_semi','count_colon']].describe())\n",
    "print()\n",
    "print('LLM')\n",
    "print(data[data['LLM_written'] == 1][['count_question','count_exclamation','count_semi','count_colon']].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "unigrams = {}\n",
    "tokenized_essays = data['essay'].progress_apply(lambda row: pytorch_tokenizer(row))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays[index]:\n",
    "        if token in unigrams[label].keys():\n",
    "            count = unigrams[label][token] + 1\n",
    "            unigrams[label][token] = count\n",
    "        else:\n",
    "            unigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unigrams_df = pd.DataFrame.from_dict(unigrams).fillna(value=0)\n",
    "unigrams_df['student_dom'] = unigrams_df['student'] - unigrams_df['llm']\n",
    "unigrams_df['llm_dom'] = unigrams_df['llm'] - unigrams_df['student']\n",
    "unigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenized_essays_bigrams = []\n",
    "for essay in tqdm(tokenized_essays):\n",
    "    tokenized_essays_bigrams.append(list(ngrams_iterator(essay,2))[len(essay):])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays_bigrams[index]:\n",
    "        if token in bigrams[label].keys():\n",
    "            count = bigrams[label][token] + 1\n",
    "            bigrams[label][token] = count\n",
    "        else:\n",
    "            bigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bigrams_df = pd.DataFrame.from_dict(bigrams).fillna(value=0)\n",
    "bigrams_df['student_dom'] = bigrams_df['student'] - bigrams_df['llm']\n",
    "bigrams_df['llm_dom'] = bigrams_df['llm'] - bigrams_df['student']\n",
    "bigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenized_essays_trigrams = []\n",
    "for essay in tqdm(tokenized_essays):\n",
    "    tokenized_essays_trigrams.append(list(ngrams_iterator(essay,3))[len(essay)*2-1:])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays_trigrams[index]:\n",
    "        if token in trigrams[label].keys():\n",
    "            count = trigrams[label][token] + 1\n",
    "            trigrams[label][token] = count\n",
    "        else:\n",
    "            trigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trigrams_df = pd.DataFrame.from_dict(trigrams).fillna(value=0)\n",
    "trigrams_df['student_dom'] = trigrams_df['student'] - trigrams_df['llm']\n",
    "trigrams_df['llm_dom'] = trigrams_df['llm'] - trigrams_df['student']\n",
    "trigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "def num_of_tokens(text:str) -> int:\n",
    "    tokenized_text = model_tokenizer(text)['input_ids']\n",
    "    return len(tokenized_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data['token_count'] = data['essay'].progress_apply(num_of_tokens)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "valid_examples = data[data['token_count'] <= 512]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, sample = train_test_split(valid_examples,test_size=1000,random_state=42,shuffle=True,stratify=valid_examples['LLM_written'])\n",
    "sample['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "emotion_predictions = []\n",
    "for essay in tqdm(sample['essay']):\n",
    "    emotion_predictions.append(classifier(essay))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample['emotion_pred'] = [exam['label'] for exam in [example[0] for example in emotion_predictions]]\n",
    "sample.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def llm_written_cat(label:int) -> str:\n",
    "    if label == 1:\n",
    "        return 'LLM'\n",
    "    else:\n",
    "        return 'student'\n",
    "sample['LLM_written_cat'] = sample['LLM_written'].progress_apply(llm_written_cat)        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.title('Emotion Prediction Per Class')\n",
    "plot = sns.countplot(sample,x='LLM_written_cat',hue='emotion_pred')\n",
    "for i in plot.containers:\n",
    "    plot.bar_label(i,)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probs_given_student = sample[sample['LLM_written'] == 0]['emotion_pred'].value_counts() / sample[sample['LLM_written'] == 0].shape[0]\n",
    "probs_given_student"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probs_given_llm = sample[sample['LLM_written'] == 1]['emotion_pred'].value_counts() / sample[sample['LLM_written'] == 1].shape[0]\n",
    "probs_given_llm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_probs = sample['emotion_pred'].value_counts() / sample.shape[0]\n",
    "sample_probs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "total_probs = sample['LLM_written'].value_counts() / sample.shape[0]\n",
    "student_given_emotion = (probs_given_student * total_probs[0]) / sample_probs \n",
    "llm_given_emotion = probs_given_llm * total_probs[1] / sample_probs \n",
    "student_given_emotion"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "llm_given_emotion"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db_conn.close()\n",
    "db_engine.dispose()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
