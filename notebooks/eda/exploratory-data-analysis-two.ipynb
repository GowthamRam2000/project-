{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis 2\n",
    "\n",
    "In this first notebook, I looked at differences in the overall essays from each class. In this notebook, I dive deeper and ask questions about the physical words and the language structure of each essay."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:21.861629Z",
     "start_time": "2024-09-22T12:22:56.071432Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "import spacy\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Adding the credentials\n",
    "sys.path.append('../')\n",
    "from credentials import credentials\n",
    "\n",
    "# Making pandas tqdm\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:22.071193Z",
     "start_time": "2024-09-22T12:23:21.862495Z"
    }
   },
   "source": [
    "# Creating the database engine \n",
    "connector_string = f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}/AuthenticAI'\n",
    "db_engine = sqlalchemy.create_engine(connector_string,echo=True)\n",
    "\n",
    "# Connecting to the database\n",
    "db_conn = db_engine.connect()"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Credentials' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Creating the database engine \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m connector_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmysql+mysqlconnector://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcredentials[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/AuthenticAI\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m db_engine \u001B[38;5;241m=\u001B[39m sqlalchemy\u001B[38;5;241m.\u001B[39mcreate_engine(connector_string,echo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Connecting to the database\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'Credentials' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique words per essay?\n",
    "\n",
    "According to papers, it seems that human text tends to have more unique words than LLM generated text. Furthermore, the ratio of unique words to total words tends to be higher. Researchers attribute this result to LLMs being more like dreams/mimickers of human text so they are very rule based and not \"conscious\". In this section, I see how many unique words each essay has, what the ratio to unique words to total words is for each essay. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T12:23:22.074372Z",
     "start_time": "2024-09-22T12:23:22.074308Z"
    }
   },
   "source": [
    "# Getting the tokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a function to get the unique words\n",
    "def get_unique_words(text:str) -> int:\n",
    "    # Tokenize the text\n",
    "    tokenized = set(tokenizer.tokenize(text))\n",
    "    return len(tokenized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the data \n",
    "data = pd.DataFrame([row for row in db_conn.execute(sqlalchemy.text('select * from essays;'))])\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Applying the unique word count function\n",
    "data['unique_word_count'] = data['essay'].progress_apply(get_unique_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a bar plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='unique_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts for Each Class')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot shows that the LLM written essays have little outliers in terms of the number of unique words whilst the student essays have a lot of outliers in terms of unique words in the essay. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Looking at the describe statistics \n",
    "print('Student Unique Words')\n",
    "print(data[data['LLM_written'] == 0]['unique_word_count'].describe())\n",
    "print()\n",
    "print('LLM Unique Words')\n",
    "print(data[data['LLM_written'] == 1]['unique_word_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the descriptive statistics, I see that the student essays tend to have a higher average number of unique words. However, this value may be skewed due to many outliers. Specifically, one essays seems to have as many as 2708 unique words. That is much higher than the the LLM essays. THe LLM essays, on the other hand, have much lower number of unique words as evidenced by the mean and standard deviation. Furthermore, the maximum number of unique words for a LLM is lower. However, is this because that student essays tend to be longer? Obviously, if an essay is longer there are more opportunities to include unique words. To close off this analysis, I will need to look at the ratio between unique words and total words for each essay."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data['unique_to_total'] = data['unique_word_count'] / data['word_count']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a bar plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='unique_to_total',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts to Total Words Ratio for Each Class')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Looking at the describe statistics \n",
    "print('Student Unique Words to Total')\n",
    "print(data[data['LLM_written'] == 0]['unique_to_total'].describe())\n",
    "print()\n",
    "print('LLM Unique Word to Total')\n",
    "print(data[data['LLM_written'] == 1]['unique_to_total'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, it seems that the first finding, student essays have more unique words than LLM essays, was flawed. The box plot shows that the LLM written essays tend to have more unique words in comparison to the total amount of words whereas a student written essay tends to have less unique words in comparison to total words. However, one flaw with this comparison is that student essays tend to be larger. Thus, for the unique word count / total word count to be high, unique word count needs to be much higher. \n",
    "\n",
    "Both experiments realize great findings but there is a great flaw in both. To mitigate this flaw, I will look at essays that are less than 400 words (the median of word count for student essays)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "smaller_word_count = data[data['word_count'] <= 400]\n",
    "smaller_word_count['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a box plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='unique_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts for Each Class for Essays <= 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a bar plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='unique_to_total',hue='LLM_written')\n",
    "plt.title('Box Plot of Unique Word Counts to Total Word Counts for Each Class for Essays <= 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "If I cap the essay word at 400, I see that I am left with about 15000 student essays and 12000 LLM written essays. I chose 400 since that is the median of the word count of the student essays. If I perform the same analysis, I can see that LLM written essays tend to have more unique words and a higher unique word ratio. While the first experiment showed that student written essays tend to have more unique words. This finding can be attributed to the fact that student written essays are much longer than LLM written ones. Hence, there is more room to add words. \n",
    "\n",
    "TLDR: A high unique word count + a high unique word count / total_word_count indicates that the essay might be written by a LLM. I can utilize these 2 findings as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many stop words does per essay? Is there a difference between a LLM and a student written essay?\n",
    "\n",
    "Stop words are defined as \"filler\" words such as the, a, I. These words don't provide much context to a piece of text. In this experiment, I see if there is a difference in the number of stop words for a LLM and a student written essay. I also see the ratio of stop words to total words for each essay to determine how much of a factor total_word_count makes. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the list of stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a function to count the stop words for each essay\n",
    "def stop_word_count(text:str) -> int:\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    count = 0\n",
    "\n",
    "    for word in tokenized:\n",
    "        if word in stop_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the stop word count\n",
    "data['stop_word_count'] = data['essay'].progress_apply(stop_word_count)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a box plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='stop_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Looking at the describe statistics \n",
    "print('Student Stop Words')\n",
    "print(data[data['LLM_written'] == 0]['stop_word_count'].describe())\n",
    "print()\n",
    "print('Student Stop Word')\n",
    "print(data[data['LLM_written'] == 1]['stop_word_count'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stop word ratio\n",
    "data['stop_word_ratio'] = data['stop_word_count'] / data['word_count']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a box plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data,x='stop_word_ratio',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Word Ratio')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this boxplot, it is clear that student essays include a higher ratio of stop words/total words. However, there are a lot of outliers that fall outside the first quartile. Initially, it seems like student essays tend to include more stop words. However, there are a lot of outliers. I want to see how much word_count impacts this. Will perform the same experiment on essays <= 400 words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "smaller_word_count = data[data['word_count'] <= 400]\n",
    "smaller_word_count['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a box plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='stop_word_count',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words for Essays less than 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a box plot\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(smaller_word_count,x='stop_word_ratio',hue='LLM_written')\n",
    "plt.title('Box Plot of Stop Words/Total Words for Essays less than 400 words')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "This experiment leads to simple results: Students tend to utilize more stop words than LLMs. This conclusion can be drawn when we look at the distribution of the stop_word_count for both classes. Even when we cap the word_count at 400, we can see that the median of stop word count is higher in the student essays. Furthermore, there is a higher ratio of stop words to total words in the student essays. This leads me to conclude that student essays tend to incorporate more stop words. This makes sense since the last experiment showed that LLMs tend to incorporate more unique words. \n",
    "\n",
    "TLDR: Include columns for stop word count and stop word to total word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the counts of punctuation differ?\n",
    "\n",
    "In this section, I want to analyze how punctuation differs between the 2 classes. Do students utilize more diverse punctuation? Is there is difference in the counts between each class?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the tokenizer\n",
    "pytorch_tokenizer = get_tokenizer('spacy',language='en_core_web_sm')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Functions for counting punctuations\n",
    "# (?, !, ;, :)\n",
    "def count_punc(text: str) -> int:\n",
    "    tokenized_text = pytorch_tokenizer(text)\n",
    "    count_q = 0\n",
    "    count_ex = 0\n",
    "    count_semi = 0\n",
    "    count_col = 0\n",
    "    for token in tokenized_text:\n",
    "        if token == \"?\":\n",
    "            count_q += 1\n",
    "        elif token == \"!\":\n",
    "            count_ex += 1\n",
    "        elif token == \";\":\n",
    "            count_semi += 1\n",
    "        elif token == \":\":\n",
    "            count_col += 1\n",
    "    \n",
    "    return count_q, count_ex,count_semi, count_col"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making columns and adding to the dataframe\n",
    "counts = data['essay'].progress_apply(count_punc)\n",
    "data['count_question'] = [row[0] for row in counts]\n",
    "data['count_exclamation'] = [row[1] for row in counts]\n",
    "data['count_semi'] = [row[2] for row in counts]\n",
    "data['count_colon'] = [row[3] for row in counts]\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Student')\n",
    "print(data[data['LLM_written'] == 0][['count_question','count_exclamation','count_semi','count_colon']].describe())\n",
    "print()\n",
    "print('LLM')\n",
    "print(data[data['LLM_written'] == 1][['count_question','count_exclamation','count_semi','count_colon']].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the board, students tend to utilize every character of punctuation more. The means are higher, but the medians are the same. What is curious is that the max counts for these marks are much higher for students than for LLMs. This means that students tend to use punctuation more. If we compare some of them, we can see that for question marks and semi-colons, the mean is much higher for the student essays. The student essays also have a higher standard deviation for each mark. This means that the counts are more spread, leading me to believe that they can get very high in comparison to the LLM essays. \n",
    "\n",
    "TLDR: The counts of each the punctuation marks matter. Can use this as a feature. The higher the punctuation mark count, the more likely for a student essay. More question marks and semi-colons are big predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What n-grams separate each essay?\n",
    "\n",
    "In this section, I want to see what n-grams separate each essay. Are there words and phrases that appear more in LLM essays than in student essays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams\n",
    "\n",
    "I start with unigrams and work my way till tri-grams. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# I can tokenize each essay and store the unigram counts in 2 fold, one for the student and one for the llm\n",
    "unigrams = {}\n",
    "tokenized_essays = data['essay'].progress_apply(lambda row: pytorch_tokenizer(row))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iterating through each tokenized essay to get the unigrams\n",
    "unigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays[index]:\n",
    "        if token in unigrams[label].keys():\n",
    "            count = unigrams[label][token] + 1\n",
    "            unigrams[label][token] = count\n",
    "        else:\n",
    "            unigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unigrams_df = pd.DataFrame.from_dict(unigrams).fillna(value=0)\n",
    "unigrams_df['student_dom'] = unigrams_df['student'] - unigrams_df['llm']\n",
    "unigrams_df['llm_dom'] = unigrams_df['llm'] - unigrams_df['student']\n",
    "unigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, I didn't learn much other than the fact that the student essays tend to have more commas. All the other tokens are stop words or \\n\\n. There isn't really a word or set of words that differentiate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table doesn't show me anything I didn't already know. I knew that the LLM essays has more unique words than the student essays. But I do notice that a LLM tends to use the words \"potential\" and \"Face\" a lot more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the tokenized essays by bigrams\n",
    "tokenized_essays_bigrams = []\n",
    "for essay in tqdm(tokenized_essays):\n",
    "    tokenized_essays_bigrams.append(list(ngrams_iterator(essay,2))[len(essay):])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iterating through each tokenized essay to get the bigrams\n",
    "bigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays_bigrams[index]:\n",
    "        if token in bigrams[label].keys():\n",
    "            count = bigrams[label][token] + 1\n",
    "            bigrams[label][token] = count\n",
    "        else:\n",
    "            bigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bigrams_df = pd.DataFrame.from_dict(bigrams).fillna(value=0)\n",
    "bigrams_df['student_dom'] = bigrams_df['student'] - bigrams_df['llm']\n",
    "bigrams_df['llm_dom'] = bigrams_df['llm'] - bigrams_df['student']\n",
    "bigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bigrams, the student essays are still just a combination of stop words like \"to be\" or \"to do\". Again, not a new finding."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams present in the LLMs more often seem to have more meaning. We see things like \"In conclusion\" or \"Additionally\" pop up more. We also see parts of the prompts present as well. Let's see if tri-grams are better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the tokenized essays by tri-grams\n",
    "tokenized_essays_trigrams = []\n",
    "for essay in tqdm(tokenized_essays):\n",
    "    tokenized_essays_trigrams.append(list(ngrams_iterator(essay,3))[len(essay)*2-1:])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iterating through each tokenized essay to get the trigrams\n",
    "trigrams = {'student':{},'llm':{}}\n",
    "labels = data['LLM_written'].tolist()\n",
    "for index in tqdm(range(len(labels))):\n",
    "    if labels[index] == 0:\n",
    "        label = 'student'\n",
    "    else:\n",
    "        label = 'llm'\n",
    "    for token in tokenized_essays_trigrams[index]:\n",
    "        if token in trigrams[label].keys():\n",
    "            count = trigrams[label][token] + 1\n",
    "            trigrams[label][token] = count\n",
    "        else:\n",
    "            trigrams[label][token] = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trigrams_df = pd.DataFrame.from_dict(trigrams).fillna(value=0)\n",
    "trigrams_df['student_dom'] = trigrams_df['student'] - trigrams_df['llm']\n",
    "trigrams_df['llm_dom'] = trigrams_df['llm'] - trigrams_df['student']\n",
    "trigrams_df.sort_values(by='student_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trigrams_df.sort_values(by='llm_dom',ascending=False).head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting. As we can see both the LLM essays and student essays tend to utilize first-person. We can see trigrams such as \"I believe that\" and \"I think that\" as popular ones in both groups. The LLM essays seem to restate prompts more and follow a classic essay structure. \"In conclusion\" is the most popular trigrams. I wonder if adding a prompt name feature helps. I imagine that this would help detect LLM essays. I see that LLM essays contain key words found in prompts. We can see that they include things like \"the Electoral College\" for the electoral college prompt. However, this method may only work for prompts found in the dataset. Additional prompts may cause the model to go awry. \n",
    "\n",
    "However, what is clear from this analysis is that the more I extend the ngrams (unigram to bigram to trigram), the more clear is becomes that the LLM essays have more structure and less stop words. The words in the LLM trigrams seem to have more \"meaning\" than the words in the student trigrams. If I need to utilize a deep learning approach, I think using a trigram vocabulary would be something to test (a vocabulary of trigrams, unigrams, and bigrams). This is because if I utilize this type of vocabulary, I can hope to capture some of the structure present in LLM essays and missing in student ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does emotion play a role?\n",
    "\n",
    "Some papers have mentioned that LLMs sometimes are devoid of emotion meaning that the texts they produce can be somewhat neutral. Furthermore, LLMs are less likely to produce texts that present negative emotions such as anger due to guardrails placed. In my last experiment, I want to see if emotion changes in the essays. Do student essays have a wider variety of emotions present? To do this, I utilize the Emotion English DistilRoBERTa-base model available on Hugging Face.\n",
    "\n",
    "I will utilize a sample of 1000 random examples stratified from labels to see this since it will take a while for the model to make all the predictions without a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "def num_of_tokens(text:str) -> int:\n",
    "    tokenized_text = model_tokenizer(text)['input_ids']\n",
    "    return len(tokenized_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the tokenized text for each text\n",
    "data['token_count'] = data['essay'].progress_apply(num_of_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Selecting the examples that match the token count\n",
    "valid_examples = data[data['token_count'] <= 512]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, sample = train_test_split(valid_examples,test_size=1000,random_state=42,shuffle=True,stratify=valid_examples['LLM_written'])\n",
    "sample['LLM_written'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Getting the model\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "emotion_predictions = []\n",
    "for essay in tqdm(sample['essay']):\n",
    "    emotion_predictions.append(classifier(essay))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample['emotion_pred'] = [exam['label'] for exam in [example[0] for example in emotion_predictions]]\n",
    "sample.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Converting LLM_written to categories\n",
    "def llm_written_cat(label:int) -> str:\n",
    "    if label == 1:\n",
    "        return 'LLM'\n",
    "    else:\n",
    "        return 'student'\n",
    "sample['LLM_written_cat'] = sample['LLM_written'].progress_apply(llm_written_cat)        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Making a histogram\n",
    "plt.title('Emotion Prediction Per Class')\n",
    "plot = sns.countplot(sample,x='LLM_written_cat',hue='emotion_pred')\n",
    "for i in plot.containers:\n",
    "    plot.bar_label(i,)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# How the emotions are broken up for students\n",
    "probs_given_student = sample[sample['LLM_written'] == 0]['emotion_pred'].value_counts() / sample[sample['LLM_written'] == 0].shape[0]\n",
    "probs_given_student"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# How the emotions are broken up for LLM\n",
    "probs_given_llm = sample[sample['LLM_written'] == 1]['emotion_pred'].value_counts() / sample[sample['LLM_written'] == 1].shape[0]\n",
    "probs_given_llm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Overall probabilities\n",
    "sample_probs = sample['emotion_pred'].value_counts() / sample.shape[0]\n",
    "sample_probs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using bayes rule to find P(y = student | emotion) and P(y = llm | emotion)\n",
    "total_probs = sample['LLM_written'].value_counts() / sample.shape[0]\n",
    "student_given_emotion = (probs_given_student * total_probs[0]) / sample_probs \n",
    "llm_given_emotion = probs_given_llm * total_probs[1] / sample_probs \n",
    "student_given_emotion"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "llm_given_emotion"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "From the plot, I identified that student essays tend to have a lot more anger, surprise, and sadness emotions predicted. From this finding, I decided to utilize Bayes Rule to find P(Y|Emotion) for each Y and each emotion. The probabilities I found lined up with the findings from the plot: If an essay is predicted to have an angry tone, there is almost guranteed to be a student essay. The same can be said for sadness and surprise. For fear, whilst there are no gurantees, there is a higher chance for an essay to be a student's essay if it is predicted to exhibit fear. Disgust, joy, and neutral are relatively the same probabilities. \n",
    "\n",
    "TLDR: I need to add categorical features that mark whether an essay exhibits anger, surprise, sadness or fear. I found this to be suitable predictors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Closing connections and deleting the engine\n",
    "db_conn.close()\n",
    "db_engine.dispose()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
