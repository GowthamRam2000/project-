{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 7508268,
     "sourceType": "datasetVersion",
     "datasetId": 4352274
    },
    {
     "sourceId": 7541711,
     "sourceType": "datasetVersion",
     "datasetId": 4351877
    }
   ],
   "dockerImageVersionId": 30646,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm \nimport torch\nimport torch.nn as nn\nimport torchtext\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom nltk.stem import SnowballStemmer\nimport re\nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score\nimport math\n\ntqdm.pandas()\n\n# Getting the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv('../input/training-llm-competition/train.csv')\n",
    "valid_data = pd.read_csv('../input/training-llm-competition/validation.csv')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "contractions = {\n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n",
    "stemmer = SnowballStemmer(language='english')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(essay:str):\n",
    "    preprocessed_essay = essay.lower()\n",
    "    \n",
    "    for contraction in contractions.keys():\n",
    "        preprocessed_essay = re.sub(contraction.lower(),contractions[contraction].lower(),preprocessed_essay)\n",
    "    \n",
    "    preprocessed_essay = re.sub(\"\\n\",\"\",preprocessed_essay)\n",
    "    preprocessed_essay = re.sub(\"\\t\",\"\",preprocessed_essay)\n",
    "\n",
    "    preprocessed_essay = preprocessed_essay.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    final_preprocessed_essay = []\n",
    "    \n",
    "    for token in tokenizer(preprocessed_essay):\n",
    "        temp_token = token.strip(\" \")\n",
    "        \n",
    "        if temp_token != \"\":\n",
    "            final_preprocessed_essay.append(stemmer.stem(token))\n",
    "    \n",
    "    return final_preprocessed_essay"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_essays_train = train_data['essay'].progress_apply(preprocess)\n",
    "tokenized_essays_valid = valid_data['essay'].progress_apply(preprocess)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vocabulary = torch.load('../input/llm-competition-models/vocab.pt')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def put_through_vocab(essay:str) -> list:\n",
    "    return vocabulary(essay)\n",
    "\n",
    "indexed_essays_train = [put_through_vocab(essay) for essay in tokenized_essays_train]\n",
    "indexed_essays_valid = [put_through_vocab(essay) for essay in tokenized_essays_valid]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_padded = tf.keras.utils.pad_sequences(indexed_essays_train,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])\n",
    "valid_padded = tf.keras.utils.pad_sequences(indexed_essays_valid,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_tensor = torch.from_numpy(train_padded)\n",
    "y_train_tensor = torch.from_numpy(train_data['LLM_written'].values)\n",
    "X_valid_tensor = torch.from_numpy(valid_padded)\n",
    "y_valid_tensor = torch.from_numpy(valid_data['LLM_written'].values)\n",
    "training_dataset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor,y_valid_tensor)\n",
    "training_loader = DataLoader(training_dataset,batch_size=128,shuffle=True)\n",
    "valid_loader = DataLoader(training_dataset,batch_size=128,shuffle=True)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,emb_size:int, dropout:float, maxlen:int = 500):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        den = torch.exp(-torch.arange(0,emb_size,2)*math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
    "        pos_embedding = torch.zeros((maxlen,emb_size))\n",
    "        pos_embedding[:,0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:,1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "\n",
    "    def forward(self,token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,vocab_size: int, emb_size:int,nheads:int,dim_feedforward:int,dropout:float,num_layers:int,max_length:int):\n",
    "        super().__init__()\n",
    "        self.embed_size = emb_size\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_size,padding_idx=vocabulary['<pad>'])\n",
    "        self.positional_encoder = PositionalEncoding(emb_size,dropout,max_length)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(emb_size,nheads,dim_feedforward,dropout,batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer,num_layers)\n",
    "        self.fc1 = nn.Linear(emb_size,1)\n",
    "\n",
    "    def forward(self,X,src_key_padding_mask):\n",
    "        output = self.embedding(X.long()) * math.sqrt(self.embed_size)\n",
    "        output = self.positional_encoder(output)\n",
    "\n",
    "        output = self.transformer(output,src_key_padding_mask=src_key_padding_mask)\n",
    "        output = torch.mean(output,dim=1)\n",
    "        return nn.functional.sigmoid(self.fc1(output))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_padding_mask(X):\n",
    "    return (X == vocabulary['<pad>'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(vocab_size=vocabulary.__len__(),emb_size=512,nheads=8,dim_feedforward=2048,dropout=0.2,num_layers=2,max_length=512)\n",
    "model.to(device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "LOSS = nn.BCELoss()\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(),LEARNING_RATE)\n",
    "history = []\n",
    "early_stopping_threshold = 5\n",
    "best_roc_auc = 0\n",
    "current_count = 0"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_preds = None\n",
    "    valid_preds = None\n",
    "    train_targets = None\n",
    "    valid_targets = None\n",
    "    model.train()\n",
    "    for X,y in training_loader:\n",
    "        y = y.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n",
    "        if train_preds is None:\n",
    "            train_preds = pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            train_preds = np.append(train_preds,pred.cpu().detach().numpy(),axis=0)\n",
    "\n",
    "        if train_targets is None:\n",
    "            train_targets = y.cpu().numpy()\n",
    "        else:\n",
    "            train_targets = np.append(train_targets,y.cpu().detach().numpy(),axis=0)\n",
    "\n",
    "        loss = LOSS(pred,y.view(-1,1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for X,y in valid_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(X,src_key_padding_mask=create_padding_mask(X))\n",
    "            loss = LOSS(pred,y.view(-1,1))\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        if valid_preds is None:\n",
    "            valid_preds = pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            valid_preds = np.append(valid_preds,pred.cpu().detach().numpy(),axis=0)\n",
    "\n",
    "        if valid_targets is None:\n",
    "            valid_targets = y.cpu().numpy()\n",
    "        else:\n",
    "            valid_targets = np.append(valid_targets,y.cpu().detach().numpy(),axis=0)\n",
    "\n",
    "    if roc_auc_score(valid_targets,valid_preds) - best_roc_auc > 1e-3:\n",
    "        best_roc_auc = roc_auc_score(valid_targets,valid_preds)\n",
    "        count = 0\n",
    "\n",
    "        torch.save(model.state_dict(),'2-layer-transformer-encoder.pt')\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "    print(f'----------EPOCH {epoch} loss----------')\n",
    "    print(f'Train Loss: {train_loss / len(training_loader)}')\n",
    "    print(f'Valid Loss: {val_loss / len(valid_loader)}')\n",
    "    print(f'Training ROC AUC: {roc_auc_score(train_targets,train_preds)}')\n",
    "    print(f'Validation ROC AUC: {roc_auc_score(valid_targets,valid_preds)}')\n",
    "    history.append([train_loss / len(training_loader),val_loss / len(valid_loader),roc_auc_score(train_targets,train_preds),roc_auc_score(valid_targets,valid_preds)])\n",
    "    print('--------------------------------------')\n",
    "    print()\n",
    "\n",
    "    if count == early_stopping_threshold:\n",
    "        print('Found no improvement!')\n",
    "        break"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "history_df = pd.DataFrame(history,columns=['Training Loss','Validation Loss','Training ROC AUC','Validation ROC AUC'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history_df[['Training Loss','Validation Loss']].plot(title='Loss vs. Epochs',xlabel='Epochs',ylabel='Loss')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history_df[['Training ROC AUC','Validation ROC AUC']].plot(title='ROC AUC vs. Epochs',xlabel='Epochs',ylabel='ROC AUC')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(vocab_size=vocabulary.__len__(),emb_size=512,nheads=8,dim_feedforward=2048,dropout=0.2,num_layers=2,max_length=512)\n",
    "model.load_state_dict(torch.load('../working/2-layer-transformer-encoder.pt'))\n",
    "model.to(device)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_preds = None\n",
    "    val_preds = None\n",
    "    train_targets = None\n",
    "    val_targets = None\n",
    "    for X,y in training_loader:\n",
    "        X = X.to(device)\n",
    "        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n",
    "        if train_preds is None:\n",
    "            train_preds = pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            train_preds = np.append(train_preds,pred.cpu().detach().numpy(),axis=0)\n",
    "        if train_targets is None:\n",
    "            train_targets = y.cpu().numpy()\n",
    "        else:\n",
    "            train_targets = np.append(train_targets,y.cpu().detach().numpy(),axis=0)\n",
    "    for X,y in valid_loader:\n",
    "        X = X.to(device)\n",
    "        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n",
    "        if valid_preds is None:\n",
    "            valid_preds = pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            valid_preds = np.append(valid_preds,pred.cpu().detach().numpy(),axis=0)\n",
    "        if valid_targets is None:\n",
    "            valid_targets = y.numpy()\n",
    "        else:\n",
    "            valid_targets = np.append(valid_targets,y.detach().numpy(),axis=0)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('Predictions for Transformer')\n",
    "train_score = roc_auc_score(train_targets,train_preds)\n",
    "valid_score = roc_auc_score(valid_targets,valid_preds)\n",
    "print(f'Training ROC AUC: {train_score}')\n",
    "print(f'Validation ROC AUC: {valid_score}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
